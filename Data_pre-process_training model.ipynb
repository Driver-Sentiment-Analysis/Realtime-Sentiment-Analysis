{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44b4322b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import tensorflow as tf \n",
    "import cv2\n",
    "from PIL import Image\n",
    "import glob\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.applications import VGG19\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten, Dropout, BatchNormalization, Activation\n",
    "\n",
    "\n",
    "## Video to frames \n",
    "\n",
    "\n",
    "import cv2\n",
    "# Playing video from file:\n",
    "cap = cv2.VideoCapture(\"E:\\\\Final_year Project\\\\Training Videos\\\\FEAR\\\\VID_20220928_140332.mp4\")\n",
    "#fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "\n",
    "try:\n",
    "    if not os.path.exists('data'):\n",
    "        os.makedirs('data')\n",
    "except OSError:\n",
    "    print ('Error: Creating directory of data')\n",
    "\n",
    "currentFrame = 0\n",
    "while(True):\n",
    "    # Capture frame-by-frame\n",
    "    try:\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # Saves image of the current frame in jpg file\n",
    "        #name = './data/frame' + str(currentFrame) + '.jpg'\n",
    "        print ('Creating...' , '.\\\\Training Dataset\\\\FEAR\\\\frame%d.jpg' % currentFrame)\n",
    "        cv2.imwrite('E:\\\\Final_year Project\\\\Training Dataset\\\\FEAR\\\\FEAR3(%d).jpg' % currentFrame, frame)   \n",
    "\n",
    "        # To stop duplicate images\n",
    "        currentFrame += 1\n",
    "    except:\n",
    "        break\n",
    "    \n",
    "# When everything done, release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "## Basic Face detection\n",
    "\n",
    "cap = cv2.VideoCapture(\"E:\\\\Final_year Project\\\\Training Videos\\\\HAPPY\\\\VID_20220911_111728.mp4\")\n",
    "\n",
    "face_locations = []\n",
    "while True:\n",
    "    # Grab a single frame of video\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    frame = np.array(frame, dtype=np.uint8)\n",
    "\n",
    "    # Convert the image from BGR color (which OpenCV uses) to RGB   \n",
    "    # color (which face_recognition uses)\n",
    "    rgb_frame = frame[:, :, ::-1]\n",
    "    \n",
    "    # Find all the faces in the current frame of video\n",
    "    face_locations = face_recognition.face_locations(rgb_frame)\n",
    "    for top, right, bottom, left in face_locations:\n",
    "        # Draw a box around the face\n",
    "        cv2.rectangle(frame, (left, top), (right, bottom), (0, 0,  \n",
    "        255), 2)\n",
    "    #Display the resulting image\n",
    "    cv2.imshow('Video', frame)\n",
    "    \n",
    "\n",
    "    # Wait for Enter key to stop\n",
    "    if cv2.waitKey(25) == 13:\n",
    "        break\n",
    "        \n",
    "\n",
    "## Pre-processing Data\n",
    "\n",
    "Datadir = \"E:\\\\Final_year Project\\\\Training Dataset\"  ##training dataset\n",
    "Classes = [\"ANGRY\", \"DISGUST\", \"FEAR\", \"HAPPY\", \"NEUTRAL\", \"SAD\",\"SURPRISED\"] #list of classes\n",
    "for category in Classes:\n",
    "    path = os.path.join(Datadir, category) ##//\n",
    "    for img in os.listdir(path):\n",
    "        img_array = cv2.imread(os.path.join(path,img))\n",
    "        plt.imshow(cv2.cvtColor(img_array,cv2.COLOR_BGR2RGB))\n",
    "        plt.show()\n",
    "        break\n",
    "    break\n",
    "img_size = 224 ##ImageNet => 224 x 224\n",
    "new_array = cv2.resize(img_array, (img_size,img_size))\n",
    "plt.imshow(cv2.cvtColor(new_array,cv2.COLOR_BGR2RGB))\n",
    "plt.show()\n",
    "\n",
    "training_data = [] ##data\n",
    "\n",
    "def create_training_Data():\n",
    "    for category in Classes:\n",
    "        path = os.path.join(Datadir, category)\n",
    "        class_num = Classes.index(category) ##0, 1 ##label\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path,img))\n",
    "                new_array = cv2.resize(img_array, (img_size,img_size))\n",
    "                training_data.append([new_array,class_num])\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(training_data)\n",
    "x = [] ##data/feature\n",
    "y = [] ##label\n",
    "\n",
    "for features,label in training_data:\n",
    "    x.append(features)\n",
    "    y.append(label)\n",
    "\n",
    "x = np.array(x).reshape(-1, img_size, img_size, 3) ## converting it to 4 dimension\n",
    "\n",
    "#normalize the data\n",
    "x = x/255.0 ; ##normalizing it ## sckit learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db97928e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep learning model for training\n",
    "\n",
    "\n",
    "\n",
    "#Initialize image data generator with rescaling\n",
    "train_data_gen=ImageDataGenerator(rescale=1./255)\n",
    "validation_data_gen=ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator=train_data_gen.flow_from_directory(\n",
    "              'E:\\\\Final_year Project\\\\Training Dataset',\n",
    "              target_size=(48, 48),\n",
    "              batch_size=64,\n",
    "              color_mode=\"grayscale\",\n",
    "              class_mode='categorical')\n",
    "\n",
    "validation_generator=validation_data_gen.flow_from_directory(\n",
    "            'E:\\\\Final_year Project\\\\Testing Dataset',\n",
    "             target_size=(48, 48),\n",
    "             batch_size=64,\n",
    "             color_mode=\"grayscale\",\n",
    "             class_mode='categorical')\n",
    "\n",
    "#create AlexNETmodel structure\n",
    "emotion_model=Sequential()\n",
    "emotion_model.add(Conv2D(32, kernel_size=(3, 3),activation='relu',padding = 'Same',input_shape=(48,48,1)))\n",
    "\n",
    "emotion_model.add(Conv2D(64,kernel_size=(3, 3), activation='relu',padding = 'Same',))\n",
    "emotion_model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "emotion_model.add(Dropout(0.25))\n",
    "\n",
    "emotion_model.add(Conv2D(128, (3, 3), activation='relu',padding = 'Same'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "emotion_model.add(Conv2D(256, (3, 3), activation='relu',padding = 'Same'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "emotion_model.add(Dropout(0.40))\n",
    "\n",
    "emotion_model.add(Conv2D(512, (3, 3), activation='relu',padding = 'Same'))\n",
    "emotion_model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "emotion_model.add(Flatten())\n",
    "emotion_model.add(Dense(1024, activation='relu'))\n",
    "emotion_model.add(Dropout(0.5))\n",
    "emotion_model.add(Dense(7, activation='softmax'))\n",
    "\n",
    "emotion_model.compile(loss='categorical_crossentropy', optimizer=Adam(learning_rate=0.0001, decay=1e-6), metrics=['accuracy'], experimental_run_tf_function=False)\n",
    "\n",
    "#Train the neural network/model\n",
    "emotion_model_info=emotion_model.fit(\n",
    "     train_generator,\n",
    "     steps_per_epoch=18670 // 64,\n",
    "     epochs = 15,\n",
    "     validation_data=validation_generator,\n",
    "     validation_steps=11200 // 64)\n",
    "\n",
    "#save model structure in json file\n",
    "model_json=emotion_model.to_json()\n",
    "with open(\"C:\\\\Users\\\\Lenovobhavya\\\\Documents\\\\Project_PBL\\\\data\\\\emotion_model_new.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "#save trained model weight in .h5 file\n",
    "emotion_model.save_weights('C:\\\\Users\\\\Lenovobhavya\\\\Documents\\\\Project_PBL\\\\data\\\\emotion_model_new.h5')\n",
    "\n",
    "\n",
    "\n",
    "emotion_model.summary()\n",
    "\n",
    "checkpoint=ModelCheckpoint(\"C:\\\\Users\\\\Lenovobhavya\\\\Documents\\\\Project_PBL\\\\data\\\\emotion_model_new.h5\", monitor='val_accuracy', verbose=1, save_best_only=True, mode= 'max')\n",
    "early_stopping=EarlyStopping(monitor='val_loss',\n",
    "                            min_delta=0,\n",
    "                            patience=3,\n",
    "                            verbose=1,\n",
    "                            restore_best_weights=True)\n",
    "reduce_learningrate = ReduceLROnPlateau(monitor= 'val_loss',\n",
    "                                factor = 0.2,\n",
    "                                patience=3,\n",
    "                                 verbose = 1,\n",
    "                                min_delta=0.0001)\n",
    "callbacks_list = [early_stopping, checkpoint, reduce_learningrate]\n",
    "\n",
    "history = emotion_model.fit(\n",
    "     train_generator,\n",
    "     steps_per_epoch=7779 // 64,\n",
    "     epochs = 15,\n",
    "     validation_data=validation_generator,\n",
    "     validation_steps=7519 // 64,\n",
    "     callbacks = [early_stopping, checkpoint, reduce_learningrate])\n",
    "print(\"Done!\")\n",
    "\n",
    "plt.figure (figsize=(20,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.suptitle( 'Optimizer:Adam', fontsize=10)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.plot(history.history['loss'], label= 'Training Loss')\n",
    "plt.plot(history.history['val_loss'], label= 'Validation Loss')\n",
    "plt.legend (loc='upper right')\n",
    "plt.subplot (1, 2, 2)\n",
    "plt.ylabel('Accuracy')\n",
    "plt.plot(history.history['accuracy'], label= 'Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label= 'Validation Accuracy')\n",
    "plt.legend (loc= 'lower right')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
